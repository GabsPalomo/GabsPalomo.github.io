{
  "hash": "10f32191fc681e83a9fe730c1a4851ee",
  "result": {
    "markdown": "---\ntitle: \"Introduction to Data Manipulation in R using the Tidyverse\"\nauthor: \"Gabriela Palomo & Hannah Griebling\"\nformat: html\ntoc: true\ntheme: [\"slides/data_manipulation/tutorial/output_style.scss\"]\n---\n\n\n\n\n## Learning objectives\n\n- After today's lecture, you'll be able to: \n\n  - Understand the structure of tidy data   \n  - Understand the main tidy verbs in dplyr to help tidy data \n  - Organize and clean data downloaded from UWIN to run a single species single season occupancy analysis \n\n## Packages\n\nThese are the packages that we are going to be working with for this tutorial. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr) # grammar of data manipulation using set of verbs; tidyverse \nlibrary(tidyr) # tidy data; tidyverse\nlibrary(readr) # reads csv files; tidyverse\nlibrary(magrittr) # has the original pipe operator %>%\nlibrary(janitor)\n```\n:::\n\n\n## Organize the project and directory\n\nPerhaps you are used to starting by setting your directory using `setwd()`. However, I highly recommend you use RStudio Projects. RStudio projects make it straightforward to divide your work into multiple contexts, each with their own working directory, workspace, history, and source documents.\n\nWe are going to start by creating a Project in RStudio. A Project is essentially a directory which will contain all the files you need for a specific project. It will have a `*.RProj` file associated with it to begin with. \n\nGo to RStudio and click on File > New Project. \n\n![](./images/new_project.png){width=4in}\n\nNow you see three options:  \n\n  - **New directory**: choose this option if you want to create a folder that will contain all the subdirectories and files of this particular project. \n  - **Existing directory**: use this option if you already created a folder which will contain all the subdirectories and files for this particular project. Choose that folder here. \n  - **Version Control**: choose this option if you are going to work with a repository already stored in GitHub. \n\nFor our own project, let's go ahead and choose 'New Directory' and let's name our project: '2024-data-manipulation-UWIN'\n\n### Other files inside the main directory \n\nYou will have a series of directories inside your project, depending on the type of work that you'll be working on. Some people recommend following the same structure that you would use if creating an r package. However, I think that at a minimum, you could have the following structure: \n\n![](./images/dir_str.png){width=2in}\n\n  * **Data** is a directory that has all your original .csv files with the data that you will use in your analysis. \n  * **Functions** is a directory that houses all the functions you create and that you will be using throughout your analysis. Some people include this directory as a subdirectory of R. \n  * **Plots** is a directory in which you will put all the graphs you create as part of your analysis. \n  * **R** is a directory that will have all the scripts needed for your analysis. \n  * **Results** is a directory that you may or may not need. The idea is to include all the resulting .csv or .rds files in here and keep them separate from your original files. \n  * You may need other directories, especially if you are working with spatial data, for example, shapefiles, rasters, maps, etc. \n\n\n## Naming files \n\nNow we should discuss a very important topic which is **naming files**.        \n  1. File names should be **machine readable**: avoid spaces, symbols, and special characters. Don’t rely on case sensitivity to distinguish files.  \n  2. File names should be **human readable**: use file names to describe what’s in the file.  \n  3. File names should play well with default ordering: start file names with numbers so that alphabetical sorting puts them in the order they get used.  \n\nHere are a few examples of **bad names**: \n\n  - `Document 1.docx`\n  - `manuscript_final.docx`\n  - `final_document_final.qmd`\n  - `data.csv`\n  \nHere are a few examples of **good names**: \n\n  * `2024_05_03_manuscript_name.R`\n  * `01_data_cleaning.R`\n  * `02_model.R`\n  * `fig-01.png`\n  * `exercise-uwin-workshop.qmd` \n  \nWhy are these **good names**? Well because if you have several of those, you can arrange them by date (descending or ascending), or by order of fig-01, fig-02. \n\n::: {.callout-warning collapse='false'}\n\nIt's important to note that `fig-01.png` is not the same as `fig-1.png` because your computer will read the following files in this order: `fig1.png`, `fig10.png`, `fig11.png`, `fig2.png`. \n\n:::\n\n## Let's talk about pipes \n\n  - At the beginning there was only one [pipe operator](https://magrittr.tidyverse.org/reference/pipe.html), `%>%`, which is from the `magrittr` package. \n\n  - The idea is to have a way to pipe an object forward into a function or call expression. \n\n  - It should be read as 'then'. For example: The following code is read as follows: start with object df THEN select col1. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>% select(col1)\n```\n:::\n\n\n![](./images/magrittr_pipe.png){height=2.5in fig-align=\"center\"}\n\n## Native pipe in base R \n\n  - Now, base R has it's own pipe called native pipe, `|>`, which is also read as 'then'.  \n  \n  - You can activate this native pipe by going to Tools > Global options > Code and selecting that option. \n\n![](./images/pipe.png){height=3in fig-align=\"center\"}\n\n  - You can read more about the differences between both pipes [here](https://www.tidyverse.org/blog/2023/04/base-vs-magrittr-pipe/).\n\n## `dplyr` verbs: data transformation\n\n- `dplyr` is a package based on a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges:\n\n  - `mutate()` adds new variables that are functions of existing variables\n  - `select()` picks variables based on their names\n  - `filter()` picks cases based on their values\n  - `summarise()` reduces multiple values down to a single summary\n  - `arrange()` changes the ordering of the rows\n  - `group_by()` groups variables for you to perform operations on the grouped data. Always remember to `ungroup()` once you are finished\n  \n- These can be linked together by pipes `|>` or `%>%`\n\n- Cool [cheatsheet for dplyr](https://github.com/rstudio/cheatsheets/blob/main/data-transformation.pdf)\n\n\n## `tidyr` for tidying data\n\n- The `tidyr` package has a series of functions that are named after verbs that will help you tidy and clean data. \n\n- The goal of `tidyr` is to help you create **tidy data**. Tidy data is data where:\n\n  - Each variable is a column; each column is a variable\n  \n  - Each observation is a row; each row is an observation  \n  \n  - Each value is a cell; each cell is a single value\n  \n- Cool [cheatsheet for tidyr](https://github.com/rstudio/cheatsheets/blob/main/tidyr.pdf)\n\n\n## Data organization for an occupancy analysis! \n\nFirst things first, let's see the data that we are going to be working with. To do so, let's use a super handy package called `readr` which is part of the `tidyverse`, specifically a function called `read_csv()`. The data we want is a `.csv` document and is conveniently stored in a folder called `data`. \n\n::: {.callout-important collapse='false'}\nHowever, there is something we need to be aware of before we read in the file. The report that the UWIN database generates has this information at the top of the document on the first 3 rows. We will need this information later but not for the main data set, so we are going to create two data frames, one with the information on the top rows and one with the info starting on row 4: \n\n![](./images/top_csv.png)\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nocc.data <- readr::read_csv(file = './data/OccupancyReport.csv', \n                               # Skip row 1-3\n                               skip = 3,\n                               # identify how not available data are coded, you can use c('NA', 'N/A')\n                               na = 'NA', \n                               # define column types, readr guesses the ones we don't specify\n                               col_types = c('f', 'd', 'c', 'd', 'd'))\n\n# With glimpse we can see the name of each column, type, and the first rows \ndplyr::glimpse(occ.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 50\nColumns: 41\n$ Species   <fct> Coyote, Coyote, Coyote, Coyote, Coyote, Coyote, Coyote, Coyo…\n$ Season    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Site      <chr> \"NCAP-MD-BSP1\", \"NCAP-MD-CCC1\", \"NCAP-MD-CRG1\", \"NCAP-MD-CVH…\n$ Latitude  <dbl> 38.8452, 38.9939, 38.8145, 39.0284, 38.7699, 38.9553, 38.814…\n$ Longitude <dbl> -76.9015, -77.0838, -76.9872, -77.0586, -77.0286, -76.8917, …\n$ Day_1     <dbl> NA, 0, NA, 0, NA, NA, NA, 0, NA, 0, 0, 0, NA, NA, 0, NA, NA,…\n$ Day_2     <dbl> NA, 0, NA, 0, NA, NA, NA, 0, NA, 0, 0, 0, NA, NA, 0, NA, NA,…\n$ Day_3     <dbl> NA, 0, NA, 0, NA, NA, NA, 0, NA, 0, 0, 0, NA, NA, 0, NA, NA,…\n$ Day_4     <dbl> NA, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, NA, 0, 0, 0, 0, 0,…\n$ Day_5     <dbl> 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, NA, 0, 0, 0, 1, 0, NA…\n$ Day_6     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_7     <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, NA, 0, 1, 0, 0, 0, NA…\n$ Day_8     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_9     <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 1, 0, 0, 0, NA…\n$ Day_10    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_11    <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_12    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_13    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, NA, 0, 0, 0, 0, 0, NA…\n$ Day_14    <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_15    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_16    <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_17    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 1, 0, 0, 0, NA…\n$ Day_18    <dbl> 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_19    <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_20    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_21    <dbl> 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 1, 0, 0, 0, NA…\n$ Day_22    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_23    <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_24    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_25    <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_26    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_27    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_28    <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_29    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ Day_30    <dbl> 0, NA, 0, 0, 0, 0, 0, NA, 0, NA, NA, NA, 0, NA, NA, 0, 0, NA…\n$ Day_31    <dbl> 0, NA, 0, 0, 0, 0, 0, NA, 0, NA, NA, NA, 0, NA, NA, 0, 0, NA…\n$ Day_32    <dbl> 0, NA, 0, 0, 0, 0, 0, NA, 0, NA, NA, NA, 0, NA, NA, 0, 0, NA…\n$ Day_33    <dbl> NA, NA, NA, NA, NA, 0, NA, NA, 0, NA, NA, NA, NA, NA, NA, 0,…\n$ Day_34    <dbl> NA, NA, NA, NA, NA, 0, NA, NA, 0, NA, NA, NA, NA, NA, NA, 0,…\n$ Day_35    <dbl> NA, NA, NA, NA, NA, 0, NA, NA, 0, NA, NA, NA, NA, NA, NA, 0,…\n$ Day_36    <dbl> NA, NA, NA, NA, NA, 0, NA, NA, 0, NA, NA, NA, NA, NA, NA, 0,…\n```\n:::\n:::\n\n\nNow let's extract the Start and End dates, we may need them later on. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nocc.info <- read_csv(file = './data/OccupancyReport.csv', \n                        n_max = 1) |> \n  select('Start Date', 'End Date')\n  \nglimpse(occ.info)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1\nColumns: 2\n$ `Start Date` <chr> \"1/10/2022\"\n$ `End Date`   <chr> \"2/14/2022\"\n```\n:::\n:::\n\n\n\n::: {.callout-tip collapse='false'}\nLet's look at the column names. They start with a capital letter and are separated by an underscore `_`. Let's review the current name conventions: \n\n![](./images/case_con.png){width=3in}\n\nThis is important to remember so that you (and your team) can always stick to a name convention to make things easier for everyone. Whichever you use, DO NOT USE A SPACE TO SEPARATE WORDS. \n\n* These are ok for the name of a column: 'day01', 'Day_1',  'day1', 'day-01', 'day-1'\n* This is not ok: 'day 1'\n\n:::\n\nYou can leave the column names as is, but I want to show you a super handy function in package `janitor` that can help us rename all the columns to fit one naming convention. \nThe options for `case` are 'snake', 'lower_camel', 'title', 'upper_camel'. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nocc.data <- janitor::clean_names(dat = occ.data, \n                                 case = 'snake')\nglimpse(occ.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 50\nColumns: 41\n$ species   <fct> Coyote, Coyote, Coyote, Coyote, Coyote, Coyote, Coyote, Coyo…\n$ season    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ site      <chr> \"NCAP-MD-BSP1\", \"NCAP-MD-CCC1\", \"NCAP-MD-CRG1\", \"NCAP-MD-CVH…\n$ latitude  <dbl> 38.8452, 38.9939, 38.8145, 39.0284, 38.7699, 38.9553, 38.814…\n$ longitude <dbl> -76.9015, -77.0838, -76.9872, -77.0586, -77.0286, -76.8917, …\n$ day_1     <dbl> NA, 0, NA, 0, NA, NA, NA, 0, NA, 0, 0, 0, NA, NA, 0, NA, NA,…\n$ day_2     <dbl> NA, 0, NA, 0, NA, NA, NA, 0, NA, 0, 0, 0, NA, NA, 0, NA, NA,…\n$ day_3     <dbl> NA, 0, NA, 0, NA, NA, NA, 0, NA, 0, 0, 0, NA, NA, 0, NA, NA,…\n$ day_4     <dbl> NA, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, NA, 0, 0, 0, 0, 0,…\n$ day_5     <dbl> 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, NA, 0, 0, 0, 1, 0, NA…\n$ day_6     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_7     <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, NA, 0, 1, 0, 0, 0, NA…\n$ day_8     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_9     <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 1, 0, 0, 0, NA…\n$ day_10    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_11    <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_12    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_13    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, NA, 0, 0, 0, 0, 0, NA…\n$ day_14    <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_15    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_16    <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_17    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 1, 0, 0, 0, NA…\n$ day_18    <dbl> 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_19    <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_20    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_21    <dbl> 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 1, 0, 0, 0, NA…\n$ day_22    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_23    <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_24    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_25    <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_26    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_27    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_28    <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_29    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA…\n$ day_30    <dbl> 0, NA, 0, 0, 0, 0, 0, NA, 0, NA, NA, NA, 0, NA, NA, 0, 0, NA…\n$ day_31    <dbl> 0, NA, 0, 0, 0, 0, 0, NA, 0, NA, NA, NA, 0, NA, NA, 0, 0, NA…\n$ day_32    <dbl> 0, NA, 0, 0, 0, 0, 0, NA, 0, NA, NA, NA, 0, NA, NA, 0, 0, NA…\n$ day_33    <dbl> NA, NA, NA, NA, NA, 0, NA, NA, 0, NA, NA, NA, NA, NA, NA, 0,…\n$ day_34    <dbl> NA, NA, NA, NA, NA, 0, NA, NA, 0, NA, NA, NA, NA, NA, NA, 0,…\n$ day_35    <dbl> NA, NA, NA, NA, NA, 0, NA, NA, 0, NA, NA, NA, NA, NA, NA, 0,…\n$ day_36    <dbl> NA, NA, NA, NA, NA, 0, NA, NA, 0, NA, NA, NA, NA, NA, NA, 0,…\n```\n:::\n:::\n\n\nHave you noticed that we have two species in our data? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nunique(occ.data$species)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] Coyote                    Eastern cottontail rabbit\nLevels: Coyote Eastern cottontail rabbit\n```\n:::\n:::\n\n\n## Dividing data into occasions  \n\nImagine we want to perform an occupancy analysis with our data. \n\nWe begin by dividing our data into occasions. In this case we have 36 days of sampling so now we want each occasion to have 7 days. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Separate only the days \ndays_oc <- occ.data |> \n  filter(species == 'Coyote')|> \n  select(contains('day_')) \n  \n# Calculate number of weeks or occasions based on 7 day groups \nn_weeks <- ceiling(ncol(days_oc)/7)\n# Create a vector where you assign the number of week to the day\nweek_groups <- rep(1:n_weeks, each = 7)[1:ncol(days_oc)]\n\n# and write a function that keeps each occasion with all NA's as such and those with all 0's as 0, and those with at least 1 detection, as 1\ncombine_days <- function(y, groups){\n  ans <- rep(NA, max(groups))\n  for(i in 1:length(groups)){\n    tmp <- as.numeric(y[groups == i])\n    if(all(is.na(tmp))){\n      next\n    } else {\n      ans[i] <- as.numeric(sum(tmp, na.rm = TRUE)>0)\n    }\n  }\n  return(ans)\n}\n\n# Apply this function across rows (in groups of 6)\ndata.weeks <- t( # this transposes our matrix\n  apply(\n    days_oc, \n    1, # 1 is for rows\n    combine_days,\n    groups = week_groups\n  )\n)\n\n# Rename columns to match number of week \ndata.wk <- data.weeks |> \n  as.data.frame() |> \n  rename_with(~paste0('week_', 1:n_weeks))\n\n# Now we can combine the species, season, site, latitude, longitude from coyote.data to our occasions.\ndata.occ <- occ.data |> \n  filter(species == 'Coyote')|> \n  select(species, season, site, latitude, longitude) |> \n  cbind(data.wk)\n```\n:::\n\n\n::: {.callout-important collapse='false'}\n\nLet's review what we just did. \nWe have 36 days, so we collapsed them all into 7-day occasions. \nIn total, we have 6 columns or occasions (weeks) but our 6th week only has 1 day worth of data.\nWe will delete it for the sake of the exercise, but make sure that you consider the ecological implications for your species of the length of your occasions. \n:::\n\n## Dashboard of our data so far\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nocc_long <- data.occ |> \n  pivot_longer(cols = starts_with('week_'), \n               names_to = 'week', \n               values_to = 'values') |> \n  mutate(values = factor(values))\n\nggplot(occ_long, aes(x = week, y = site, fill = values))+\n  geom_tile( \n            color = 'white', \n            lwd = 1, \n            linetype = 1)+\n  coord_equal()+\n  labs(title = paste0('Weekly detections by site'))+\n  scale_fill_manual(values = c('#6F9CDE', '#FC8955'), \n                    na.value = '#A9A9A9') +\n  theme(legend.position = 'left', \n        axis.text.x = element_text(angle = 90), \n        legend.title = element_blank(), \n        axis.title = element_blank(), \n        plot.title = element_text(hjust=0, face = 'bold', size = 16)) -> tile.plot \n\ntab <- as.data.frame(\n  c(Start = occ.info$`Start Date`, \n    End = occ.info$`End Date`, \n    Sites = nrow(data.occ), \n    Species = as.character(occ_long$species[[1]]), \n    Occassions = n_weeks, \n    'Days per occassion' = 7)\n)\n\nlibrary(gridExtra)\np_tab <- tableGrob(unname(tab), theme = ttheme_minimal(core=list(fg_params=list(hjust=0, x=0)),\n                                                       rowhead=list(fg_params=list(hjust=0, x=0, \n                                                                                   col = 'black'))))\ngrid.arrange(tile.plot, p_tab, ncol = 2, padding = unit(0, 'cm'))\n```\n\n::: {.cell-output-display}\n![](tutorial-data-manipulation_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n## Let's tidy the spatial covariates \n\nThese are covariates that vary at the site level. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in the file with covariates \ncovariates <- read_csv('./data/covariates.csv')\n\n# make sure column names follow snake convention \ncovariates <- janitor::clean_names(covariates, \n                                   \"snake\")\nglimpse(covariates)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 25\nColumns: 6\n$ site       <chr> \"NCAP-MD-BSP1\", \"NCAP-MD-CCC1\", \"NCAP-MD-CRG1\", \"NCAP-MD-CV…\n$ latitude   <dbl> 38.8452, 38.9939, 38.8145, 39.0284, 38.7699, 38.9553, 38.81…\n$ longitude  <dbl> -76.9015, -77.0838, -76.9872, -77.0586, -77.0286, -76.8917,…\n$ forest     <dbl> 0.5990142, 0.7361551, 0.6614681, 0.4533951, 0.7374635, 0.59…\n$ ag         <dbl> 0.19148783, 0.14867841, 0.23992384, 0.10802248, 0.09503045,…\n$ dist_water <dbl> 379.6535, 618.4484, 605.5428, 560.6045, 566.1160, 337.4902,…\n```\n:::\n:::\n\n\nFirst thing we should do is scale our covariates so we can compare across them. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncovariates.sc <- covariates |> \n  # Select only the columns you wish to scale\n  select(forest:dist_water) |>\n  # mutate across all the columsn you wish to scale \n  mutate(across(forest:dist_water, \n                ~as.vector(scale(.x)))) |> \n  # rename the columns you selected and add _scaled at the end of each one\n  rename_with(~paste0(.x, '_scaled')) |> \n  # combine all the columns here and with the file covariates\n  cbind(covariates) |> \n  # reorganize the columns in the order you want\n  relocate(site, latitude, longitude, forest, ag, dist_water, forest_scaled, ag_scaled, dist_water_scaled)\n\nglimpse(covariates.sc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 25\nColumns: 9\n$ site              <chr> \"NCAP-MD-BSP1\", \"NCAP-MD-CCC1\", \"NCAP-MD-CRG1\", \"NCA…\n$ latitude          <dbl> 38.8452, 38.9939, 38.8145, 39.0284, 38.7699, 38.9553…\n$ longitude         <dbl> -76.9015, -77.0838, -76.9872, -77.0586, -77.0286, -7…\n$ forest            <dbl> 0.5990142, 0.7361551, 0.6614681, 0.4533951, 0.737463…\n$ ag                <dbl> 0.19148783, 0.14867841, 0.23992384, 0.10802248, 0.09…\n$ dist_water        <dbl> 379.6535, 618.4484, 605.5428, 560.6045, 566.1160, 33…\n$ forest_scaled     <dbl> -0.1500416, 0.6588543, 0.2183293, -1.0089438, 0.6665…\n$ ag_scaled         <dbl> 0.117804331, -0.812708927, 1.170618390, -1.696413643…\n$ dist_water_scaled <dbl> -0.56135757, 0.63584774, 0.57114511, 0.34584580, 0.3…\n```\n:::\n:::\n\n\n## Let's tidy the observational covariates \n\nThese are covariates that vary by occasions or within sites. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp_covs <- read_csv('./data/temp_covs.csv') |> \n  select(!Site)\n\n# We have to summarize it for each occasion, remember you only have 6 occasions \nlibrary(purrr)\nrow_means <- function(data) {\n  map(seq(1, ncol(data), by = 7),\n      ~rowMeans(select(data, .x:min(.x + 6, ncol(data))), na.rm = TRUE)) |> \n       set_names(paste0(\"week_\", seq(1, 6, by = 1))) |> \n       tibble::as_tibble()\n}\n\n\n# Apply the function and coerce the object to be a matrix \ntemp_avg <-  row_means(temp_covs)\n```\n:::\n\n\n\n## Now we are ready to create our `unmarkedFrameOccu()` to begin an occupancy model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(unmarked)\n\n# Detection data must only include the columns with the occasions\ny <- data.occ |> \n  select(week_1:week_6)\n\n# Site covariates must only be the columns with the scaled covariates   \nsiteCovs <- covariates.sc |> \n  select(forest_scaled:dist_water_scaled)\n\n# You need a list of matrices for the observation covariates, \n# And each matrix must be named \nobsCovs <- list(temp_avg=temp_avg)\n\noccu.df <- unmarkedFrameOccu(y = y, \n                             siteCovs = siteCovs, \n                             obsCovs = obsCovs)\n```\n:::\n\n\n\n## END\n",
    "supporting": [
      "tutorial-data-manipulation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}